{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Midway Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NCuQ7rfUMCt",
        "outputId": "d0fd41ec-fe47-40ad-816f-13e576e09b34"
      },
      "source": [
        "# import packages\n",
        "import math, re, os, random, cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from functools import partial\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OiXUj63335k",
        "outputId": "78f8b0e0-dc5b-45f8-eaf7-809813115837"
      },
      "source": [
        "# testing TPU\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of replicas: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3weuHqT359X"
      },
      "source": [
        "# setting hyperparameters\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "IMAGE_SIZE = [224, 224]\n",
        "CLASSES = ['0', '1', '2', '3', '4']\n",
        "EPOCHS = 20\n",
        "LR = 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wykYnGJe5XB4",
        "outputId": "1cb5ad87-91b4-482f-88c9-f90f4fd4c22f"
      },
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3bVDLFC5bro"
      },
      "source": [
        "path = \"/content/drive/MyDrive/TensorFlow_Bros/Kaggle\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZVqhXsYxl2l"
      },
      "source": [
        "# split data into training and validation data\r\n",
        "training_filenames, valid_filenames = train_test_split(\r\n",
        "    tf.io.gfile.glob(path + '/train_tfrecords/ld_train*.tfrec'),\r\n",
        "    test_size=0.2, random_state=5\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgR2JTi139mZ"
      },
      "source": [
        "# define functions to process TFRecords data using below website\n",
        "# https://www.kaggle.com/jsmithperera/cassava-inference\n",
        "\n",
        "def decode_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example, labeled):\n",
        "    tfrecord_format = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
        "    } if labeled else {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    image = decode_image(example['image'])\n",
        "    if labeled:\n",
        "        label = tf.cast(example['target'], tf.int32)\n",
        "        return image, label\n",
        "    idnum = example['image_name']\n",
        "    return image, idnum\n",
        "\n",
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def data_augment(image, label):\n",
        "    #image = tf.image.random_flip_left_right(image)\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oONWd-85C78r"
      },
      "source": [
        "# functions to get training, validation, and testing data set\r\n",
        "# https://www.kaggle.com/jsmithperera/cassava-inference\r\n",
        "\r\n",
        "def get_test_dataset(ordered=False):\r\n",
        "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\r\n",
        "    dataset = dataset.batch(BATCH_SIZE)\r\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\r\n",
        "    return dataset\r\n",
        "\r\n",
        "def get_training_dataset():\r\n",
        "    dataset = load_dataset(training_filenames, labeled=True)  \r\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)  \r\n",
        "    dataset = dataset.repeat()\r\n",
        "    dataset = dataset.shuffle(2048)\r\n",
        "    dataset = dataset.batch(BATCH_SIZE)\r\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\r\n",
        "    return dataset\r\n",
        "\r\n",
        "def get_validation_dataset(ordered=False):\r\n",
        "    dataset = load_dataset(valid_filenames, labeled=True, ordered=ordered) \r\n",
        "    dataset = dataset.batch(BATCH_SIZE)\r\n",
        "    dataset = dataset.cache()\r\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\r\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_YJaY5w4fST",
        "outputId": "0c9a7377-07ba-4ea9-f6ac-5bc9738b88f9"
      },
      "source": [
        "# check training/validation split\n",
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "NUM_TRAINING_IMAGES = count_data_items(training_filenames)\n",
        "NUM_VALIDATION_IMAGES = count_data_items(valid_filenames)\n",
        "\n",
        "print('Dataset: {} training images, {} validation images'.format(\n",
        "    NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 16045 training images, 5352 validation images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn6EE6j78Vdw"
      },
      "source": [
        "# import packages for model construction\r\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\r\n",
        "from keras.preprocessing import image\r\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from keras.callbacks import LearningRateScheduler\r\n",
        "from keras.layers import Dense, Dropout, Input, MaxPooling2D, ZeroPadding2D, Conv2D, Flatten\r\n",
        "from keras.models import Sequential, Model\r\n",
        "from keras.losses import categorical_crossentropy\r\n",
        "from keras.optimizers import Adam, SGD\r\n",
        "from keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\r\n",
        "from keras.utils import to_categorical\r\n",
        "from tensorflow.keras import regularizers\r\n",
        "from tensorflow.keras.layers import MaxPool2D, AveragePooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\r\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNG_e8SobYLS"
      },
      "source": [
        "# data augmentation\r\n",
        "data_augmentation = tf.keras.Sequential([\r\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip(mode='horizontal_and_vertical'),\r\n",
        "  #tf.keras.layers.experimental.preprocessing.RandomRotation(factor=(-0.125,0.125)),\r\n",
        "  #tf.keras.layers.experimental.preprocessing.RandomCrop(height=112, width=112),\r\n",
        "  #tf.keras.layers.experimental.preprocessing.Resizing(height=128, width=128)\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fYhBVnxeS14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dacf6561-4dd2-471c-bf22-cb3dfc01ad35"
      },
      "source": [
        "# function to build model\r\n",
        "def build_model():\r\n",
        "\r\n",
        "  inputshape = (224, 224, 3) # input size\r\n",
        "  inputs = tf.keras.Input(shape=inputshape)\r\n",
        "\r\n",
        "  x = data_augmentation(inputs) # apply data augmentation\r\n",
        "\r\n",
        "  base_model = ResNet50(weights = 'imagenet', include_top = False, \r\n",
        "                        input_shape=inputshape) # load base model\r\n",
        "  \r\n",
        "  # add layers to base model\r\n",
        "  x = base_model(x)\r\n",
        "  model = GlobalMaxPooling2D()(x)\r\n",
        "  model = Dense(512,activation='relu',kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(0.001))(model)\r\n",
        "  model = BatchNormalization()(model)\r\n",
        "  model = Dense(128,activation='relu',kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(0.001))(model)\r\n",
        "  model = BatchNormalization()(model)\r\n",
        "  model = Dense(32,activation='relu',kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(0.001))(model)\r\n",
        "  model = BatchNormalization()(model)\r\n",
        "  predictions = Dense(len(CLASSES), activation='softmax')(model)\r\n",
        "\r\n",
        "  model = Model(inputs = inputs, outputs = predictions)\r\n",
        "  return model\r\n",
        "\r\n",
        "model = build_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cobBGkvikYNY"
      },
      "source": [
        "# compile model\r\n",
        "model.compile(optimizer=Adam(learning_rate=LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7y3Xnw6BjKX"
      },
      "source": [
        "# set hyperparameters\r\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\r\n",
        "VALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvM7O-D9nJ6u"
      },
      "source": [
        "# get training and validation data\r\n",
        "train_dataset = get_training_dataset()\r\n",
        "valid_dataset = get_validation_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1mRc45n-nIK"
      },
      "source": [
        "# train model\r\n",
        "history = model.fit(train_dataset,\r\n",
        "                    steps_per_epoch=STEPS_PER_EPOCH,\r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    validation_data=valid_dataset\r\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEUoXeJg1mWb"
      },
      "source": [
        "# plot results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
        "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"Train Loss\")\n",
        "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.title(\"Loss and Accuracy Plot\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "plt.savefig(\"midterm_plot.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHbwUThVspGV",
        "outputId": "08d6e39b-d11c-4830-a553-1166a1fbd102"
      },
      "source": [
        "# save model\r\n",
        "# https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l07c01_saving_and_loading_models.ipynb#scrollTo=OGNpmn43C0O6\r\n",
        "\r\n",
        "import time\r\n",
        "t = time.time()\r\n",
        "\r\n",
        "export_path_keras = \"./{}.h5\".format(int(t))\r\n",
        "print(export_path_keras)\r\n",
        "\r\n",
        "model_reg.save(export_path_keras)\r\n",
        "\r\n",
        "export_path_sm = \"./{}\".format(int(t))\r\n",
        "print(export_path_sm)\r\n",
        "\r\n",
        "tf.saved_model.save(model_reg, export_path_sm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./1615691677.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8gvOT7MvE1W"
      },
      "source": [
        "# download model to computer\r\n",
        "\r\n",
        "!zip -r model.zip {export_path_sm}\r\n",
        "\r\n",
        "try:\r\n",
        "  from google.colab import files\r\n",
        "  files.download('./model.zip')\r\n",
        "except ImportError:\r\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}